{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d55c79",
   "metadata": {},
   "source": [
    "# Aмангелді Нұрғалым - Зертханалық жұмыс"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7b67b",
   "metadata": {},
   "source": [
    "## Представление данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12d185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Один обучающий/тестовый пример для классификации последовательностей\n",
    "class InputExample(object):\n",
    "    # Создает пример ввода.\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        self.guid = guid # Создает пример ввода.\n",
    "        self.text_a = text_a #Неотмеченный текст первой последовательности. \n",
    "        self.text_b = text_b # Неотмеченный текст второй последовательности.\n",
    "        self.labels = labels # Метка примера. Это должно быть указано для примеров train и dev, но не для тестовых примеров."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cdd032",
   "metadata": {},
   "source": [
    "### Этот код определяет класс InputFeatures, который служит для представления набора свойств (features) для обработки входных данных моделью машинного обучения, такой как модель BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fbaa409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Единый набор свойств данных\n",
    "class InputFeatures(object):\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc6a8f",
   "metadata": {},
   "source": [
    "## Архитектура модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662ca5db",
   "metadata": {},
   "source": [
    "## Модель BERT для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fa7ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Этот модуль состоит из наилучшей модели с линейным слоем поверх объединенных выходных данных.\n",
    "# Импорт необходимых библиотек и классов\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "import torch\n",
    "\n",
    "# Определение класса модели BERT для многозначной классификации последовательностей,\n",
    "# который наследует от PreTrainedModel из transformers\n",
    "class BertForMultiLabelSequenceClassification(PreTrainedModel):\n",
    "    def __init__(self, config, num_labels=2):\n",
    "        # Инициализация класса с использованием конфигурации и указанием числа меток\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        \n",
    "        # Задание числа меток\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Создание экземпляра модели BERT с использованием переданной конфигурации\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        # Добавление слоя dropout для предотвращения переобучения\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        # Линейный слой для классификации с выходной размерностью, равной числу меток\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "        \n",
    "        # Применение метода для инициализации весов BERT\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    # Определение forward-метода для прямого прохода модели\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        # Получение выходных данных от BERT\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        \n",
    "        # Применение dropout к выходу BERT\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Подача выхода через линейный классификатор\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        # Если есть метки, вычисление функции потерь\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss\n",
    "        else:\n",
    "            # В противном случае возвращение выхода модели\n",
    "            return logits\n",
    "        \n",
    "    # Метод для замораживания параметров BERT\n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Метод для размораживания параметров BERT\n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3bf5df",
   "metadata": {},
   "source": [
    "#### Основное изменение здесь — использование бинарной кросс-энтропийной функции потерь с logit-функцией (BCEWithLogitsLoss) вместо классической кросс-энтропийной функции, используемой для мультиклассовой классификации. Бинарная кросс-энтропийная потеря позволяет нашей модели присваивать меткам независимые вероятности."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8c6d6",
   "metadata": {},
   "source": [
    "## Код модели показывает слои модели вместе с их размерами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b99dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings, BertEncoder, BertLayer, BertAttention, BertSelfAttention, BertSelfOutput, BertIntermediate, BertOutput, BertPooler\n",
    "from torch.nn import Embedding, Linear, ModuleList, FusedLayerNorm, Dropout, Tanh\n",
    "# Определение модели BERT для многозначной классификации последовательностей\n",
    "model = BertForSequenceClassification(\n",
    "    config=BertConfig(\n",
    "        # Встраивание токенов\n",
    "        embeddings=BertEmbeddings(\n",
    "            # Встраивание слов\n",
    "            word_embeddings=Embedding(28996, 768),\n",
    "            # Встраивание позиций токенов\n",
    "            position_embeddings=Embedding(512, 768),\n",
    "            # Встраивание типов токенов\n",
    "            token_type_embeddings=Embedding(2, 768),\n",
    "            # Нормализация слоев\n",
    "            LayerNorm=FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True),\n",
    "            # Dropout для предотвращения переобучения\n",
    "            dropout=Dropout(p=0.1)\n",
    "        ),\n",
    "        # Слои энкодера BERT\n",
    "        encoder=BertEncoder(\n",
    "            # Список слоев BERT\n",
    "            layer=ModuleList(\n",
    "                # Один из 12 слоев BERT\n",
    "                BertLayer(\n",
    "                    # Механизм внимания внутри слоя\n",
    "                    attention=BertAttention(\n",
    "                        # Собственно внимание\n",
    "                        self=BertSelfAttention(\n",
    "                            # Линейные трансформации для запроса, ключа и значения\n",
    "                            query=Linear(in_features=768, out_features=768, bias=True),\n",
    "                            key=Linear(in_features=768, out_features=768, bias=True),\n",
    "                            value=Linear(in_features=768, out_features=768, bias=True),\n",
    "                            # Dropout для предотвращения переобучения\n",
    "                            dropout=Dropout(p=0.1)\n",
    "                        ),\n",
    "                        # Выход из механизма внимания\n",
    "                        output=BertSelfOutput(\n",
    "                            # Линейный слой для выхода\n",
    "                            dense=Linear(in_features=768, out_features=768, bias=True),\n",
    "                            # Нормализация слоев\n",
    "                            LayerNorm=FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True),\n",
    "                            # Dropout для предотвращения переобучения\n",
    "                            dropout=Dropout(p=0.1)\n",
    "                        )\n",
    "                    ),\n",
    "                    # Промежуточный слой с полносвязным перцептроном\n",
    "                    intermediate=BertIntermediate(\n",
    "                        dense=Linear(in_features=768, out_features=3072, bias=True)\n",
    "                    ),\n",
    "                    # Выходной слой слоя BERT\n",
    "                    output=BertOutput(\n",
    "                        # Линейный слой для выхода\n",
    "                        dense=Linear(in_features=3072, out_features=768, bias=True),\n",
    "                        # Нормализация слоев\n",
    "                        LayerNorm=FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True),\n",
    "                        # Dropout для предотвращения переобучения\n",
    "                        dropout=Dropout(p=0.1)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        # Слой пулинга для создания представления последовательности\n",
    "        pooler=BertPooler(\n",
    "            # Линейный слой для пулинга\n",
    "            dense=Linear(in_features=768, out_features=768, bias=True),\n",
    "            # Функция активации Tanh\n",
    "            activation=Tanh()\n",
    "        )\n",
    "    ),\n",
    "    # Dropout для предотвращения переобучения\n",
    "    dropout=Dropout(p=0.1),\n",
    "    # Линейный слой для классификации с заданным числом классов\n",
    "    classifier=Linear(in_features=768, out_features=6, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2877facb",
   "metadata": {},
   "source": [
    "# Метрика качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a7a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_thresh(y_pred:Tensor, y_true:Tensor, thresh:float=0.5, sigmoid:bool=True):\n",
    " # \"Вычислите точность, когда `y_pred` и `y_true` имеют одинаковый размер\".\n",
    "    if sigmoid: y_pred = y_pred.sigmoid()\n",
    "\n",
    "    return np.mean(((y_pred>thresh)==y_true.byte()).float().cpu().numpy(), axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b82e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Вычислить кривую ROC и площадь ROC для каждого класса\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(num_labels):\n",
    "    fpr[i], tpr[i], _ = roc_curve(all_labels[:, i], all_logits[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# # Компьютерная микро-усредненная кривая ROC и площадь ROC\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(all_labels.ravel(), all_logits.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2626cd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
